{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481153b7-6e77-4c85-9776-1ab3c96da7f4",
   "metadata": {},
   "source": [
    "# **Statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e919239-85b7-4dfe-96bc-93f1c336b085",
   "metadata": {},
   "source": [
    "# 1. Simulation of Random Variables and Their Properties\n",
    "\n",
    "**Objective**: Simulate random variables to understand their distributions, calculate their means and variances, and visualize the results.\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Analyze the Iris dataset to calculate the mean and variance of the features for each species of iris.\n",
    "\n",
    "I use the popular Iris dataset, which is often used for statistical testing, machine learning, and data visualization projects. This dataset includes measurements of 150 iris flowers from three species on four features: sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "- Generate samples from different distributions (uniform, normal, binomial, Poisson) using `numpy` or `scipy.stats` and plot their histograms using `matplotlib` or `seaborn`.\n",
    "- For each distribution, calculate the theoretical mean and variance, compare these with the sample mean and sample variance, and discuss the results in the context of the Law of Large Numbers.\n",
    "- Demonstrate the Central Limit Theorem by showing that the distribution of the sample mean approaches a normal distribution as sample size increases, regardless of the original distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459513e-5573-4ee4-806e-4c492b884b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Add species (target) to the DataFrame\n",
    "iris_df['species'] = iris.target\n",
    "iris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "iris_df.head()\n",
    "\n",
    "# Calculate mean and variance for each feature grouped by species\n",
    "mean_variance_df = iris_df.groupby('species').agg(['mean', 'var'])\n",
    "\n",
    "# Display the calculated mean and variance\n",
    "mean_variance_df\n",
    "\n",
    "# Set the style of seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Plot for Sepal Length\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=mean_variance_df.index, y=mean_variance_df[('sepal length (cm)', 'mean')], yerr=mean_variance_df[('sepal length (cm)', 'var')].apply(np.sqrt), capsize=.2)\n",
    "plt.title('Mean and Standard Deviation of Sepal Length for Each Species')\n",
    "plt.ylabel('Sepal Length (cm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f607eaee-f009-4753-b5a7-d163d20c12a8",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The mean tells us about the average sizes of different iris features per species, providing a quick reference for distinguishing between species based on size. The variance adds depth to this picture by revealing how much individuals within a species vary from that average, which is essential for understanding the diversity within each species.\n",
    "- **Central Tendency (Mean)**: Calculating the mean of each feature (sepal length, sepal width, petal length, petal width) for each iris species offers a snapshot of the typical size of these features within each species group. For instance, if the mean sepal length for setosa is significantly lower than that for versicolor and virginica, it suggests that, on average, setosa irises have shorter sepals. This measure of central tendency is crucial for understanding the general physical characteristics that define each species.\n",
    "- **Variability (Variance)**: The variance provides insights into the spread of each feature within the species. A high variance in sepal length for a particular species would indicate that the sepal lengths within that species vary widely, suggesting a high degree of diversity or perhaps different subpopulations within that species. Conversely, a low variance implies that the feature sizes are more uniform, indicating consistency in the physical characteristics of the flowers within that species.\n",
    "- **Comparative Analysis**: By comparing the mean and variance of these features across species, we can identify distinguishing characteristics. For example, if the virginica species shows a significantly higher mean petal length with low variance, it could be inferred that longer petals are a consistent and defining characteristic of the virginica species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef0eb2-c4b8-4864-bfc9-a5ef268a102f",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8475aebc-28a6-4310-a632-7f360d65c330",
   "metadata": {},
   "source": [
    "# 2. Image Data Analysis for Computer Vision\n",
    "\n",
    "**Objective**: Demonstrate the application of statistical concepts in processing and analyzing image data, relevant to computer vision.\n",
    "\n",
    "## Example\n",
    "\n",
    "Demonstrate how to calculate the mean image and variance across the MNIST dataset, perform image normalization and contrast adjustment, and implement a simple image classification model.\n",
    "\n",
    "MNIST dataset is ideal for demonstrating the application of statistical concepts in computer vision, as it contains images of handwritten digits (0 through 9), which are commonly used for training various image processing systems.\n",
    "\n",
    "\n",
    "*   Load an image dataset (e.g., MNIST, CIFAR-10) and calculate the mean image and variance across the dataset. Discuss how these metrics can be used in image preprocessing steps for machine learning models.\n",
    "*   For a subset of images, calculate the pixel intensity distribution's mean and variance. Use these statistics to perform image normalization and contrast adjustment.\n",
    "*   Implement a simple image classification model using a machine learning library (e.g., scikit-learn, TensorFlow, or PyTorch). Discuss how the mean and variance of image features influence the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a8741-ef37-4b67-b1e2-ec699880f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import nan_to_num\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('./mnist_train.csv')\n",
    "test_df = pd.read_csv('./mnist_test.csv')\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = train_df.drop('label', axis=1).values\n",
    "y_train = train_df['label'].values\n",
    "X_test = test_df.drop('label', axis=1).values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# Calculate mean and variance\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "variance_image = np.var(X_train, axis=0)\n",
    "\n",
    "# Normalize the training and testing set\n",
    "X_train_normalized = (X_train - mean_image) / np.sqrt(variance_image)\n",
    "X_test_normalized = (X_test - mean_image) / np.sqrt(variance_image)\n",
    "\n",
    "# Replace NaN values with 0 after normalization (to handle division by zero for pixels with no variance)\n",
    "X_train_normalized = nan_to_num(X_train_normalized, nan=0)\n",
    "X_test_normalized = nan_to_num(X_test_normalized, nan=0)\n",
    "\n",
    "# Now, it's safe to check for NaN values, though they should have been handled by the previous step\n",
    "nan_in_train = np.isnan(X_train_normalized).any()\n",
    "nan_in_test = np.isnan(X_test_normalized).any()\n",
    "\n",
    "print(f\"NaN in Training Set: {nan_in_train}\")\n",
    "print(f\"NaN in Testing Set: {nan_in_test}\")\n",
    "\n",
    "# Visualize the mean image\n",
    "plt.imshow(mean_image.reshape(28, 28), cmap='gray')\n",
    "plt.title('Mean Image')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the variance image\n",
    "plt.imshow(variance_image.reshape(28, 28), cmap='gray')\n",
    "plt.title('Variance Image')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Initialize and train the classifier\n",
    "clf = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)\n",
    "clf.fit(X_train_normalized, y_train)\n",
    "\n",
    "# Predict on the test set and calculate accuracy\n",
    "y_pred = clf.predict(X_test_normalized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7174d-04d2-4c9c-97f4-3c0b2df71e93",
   "metadata": {},
   "source": [
    "## Discuss the Influence of Mean and Variance\n",
    "\n",
    "In the process of preparing the MNIST dataset for the classification task, two critical preprocessing steps were normalization and handling of NaN values, influenced by the mean and variance calculations of the dataset. These steps had a significant impact on the model's training process and its subsequent performance. Here's a detailed discussion on their influence:\n",
    "\n",
    "### Normalization Using Mean and Variance\n",
    "\n",
    "Normalization involves adjusting the values in the dataset so that they share a common scale, without distorting differences in the ranges of values. For the MNIST dataset, normalization was achieved by subtracting the mean and dividing by the standard deviation (square root of variance) of the dataset. This method ensures that:\n",
    "\n",
    "- **Feature Scaling**: Each pixel value is scaled similarly, making the optimization landscape smoother. This is crucial for algorithms like Logistic Regression that rely on gradient descent, as it ensures more uniform convergence across all features (pixels in this context).\n",
    "- **Improved Model Performance**: By scaling the features to a similar range, models can train faster and often achieve better accuracy. The model's ability to learn from the data is enhanced because the features contribute more equally to the training process.\n",
    "- **Reduction of Bias**: Without normalization, pixel values with higher numerical ranges could dominate the model's learning process, leading to biased predictions. Normalization mitigates this risk by giving each pixel equal importance based on its variability, rather than its absolute value.\n",
    "\n",
    "### Handling NaN Values\n",
    "\n",
    "The appearance of NaN values during normalization (specifically when dividing by zero variance for pixels that do not change across all images) necessitated additional preprocessing. Replacing NaN values with zeros ensures that:\n",
    "\n",
    "- **Data Integrity**: The model receives a complete dataset without missing values, which could otherwise introduce bias or errors during training.\n",
    "- **Consistent Input**: Ensures that all inputs to the model are real numbers, which is a prerequisite for most mathematical operations involved in machine learning algorithms.\n",
    "- **Uninterrupted Training Process**: By addressing potential NaN issues upfront, the training process runs smoothly without interruption due to unexpected input values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b536e1b8-0f9c-444b-81d1-203c10be7cae",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede8243-8860-49f2-892e-4e95d717e760",
   "metadata": {},
   "source": [
    "# 3. Analysis of Variability in Weather Data\n",
    "\n",
    "## Objective:\n",
    "\n",
    "Analyze a dataset of daily temperatures to calculate mean, variance, and standard deviation, demonstrating your understanding of these concepts and their application in summarizing and understanding data variability.\n",
    "\n",
    "## Dataset:\n",
    "\n",
    "You can use any publicly available weather dataset that includes daily temperature readings. For the purpose of this exercise, let's assume you have a dataset **`daily_temperatures.csv`** with columns **`Date`** and **`Temperature`**.\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. **Load and Prepare the Data**: Import necessary libraries (**`pandas`**, **`numpy`**) and load the dataset into a DataFrame. Convert **`Date`** to a datetime type and **`Temperature`** to a float.\n",
    "2. **Calculate Descriptive Statistics**: Compute the mean and variance of **`Temperature`** using **`numpy`** or **`pandas`** functions.\n",
    "3. **Visualize the Data**: Plot the temperature over time using **`matplotlib`** or **`seaborn`** to visualize trends and variability.\n",
    "4. **Interpret the Results**: Discuss what the mean, variance, and the plot tell you about the temperature distribution and variability over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f783c0-e265-4d8e-8d4f-6a43bdc84885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and Prepare the Data\n",
    "# For this example, let's simulate loading a dataset since we can't access external files directly.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulating a dataset of daily temperatures over a year\n",
    "np.random.seed(0) # For reproducibility\n",
    "dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
    "temperatures = 20 + 10*np.sin(np.linspace(0, 3*np.pi, 365)) + np.random.normal(0, 5, 365)\n",
    "\n",
    "daily_temperatures_df = pd.DataFrame({'Date': dates, 'Temperature': temperatures})\n",
    "\n",
    "# Converting 'Date' to datetime type and 'Temperature' to float is not needed as they are already in the correct format\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "daily_temperatures_df.head()\n",
    "\n",
    "# Step 2: Calculate Descriptive Statistics\n",
    "mean_temperature = daily_temperatures_df['Temperature'].mean()\n",
    "variance_temperature = daily_temperatures_df['Temperature'].var()\n",
    "std_dev_temperature = daily_temperatures_df['Temperature'].std()\n",
    "\n",
    "(mean_temperature, variance_temperature, std_dev_temperature)\n",
    "\n",
    "# Step 3: Visualize the Data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(daily_temperatures_df['Date'], daily_temperatures_df['Temperature'], label='Daily Temperature')\n",
    "plt.axhline(mean_temperature, color='r', linestyle='--', label='Mean Temperature')\n",
    "plt.title('Daily Temperatures Over a Year')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818d0803-db9c-44f8-84f7-6374ab93f9f3",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "The plot visualizes the daily temperature fluctuations over the course of a year. By calculating the mean temperature, variance, and standard deviation, we can understand the dataset's central tendency and variability:\n",
    "\n",
    "- **Mean Temperature**: The mean provides a central value around which daily temperatures tend to cluster. In our simulated data, this is visually represented by the red dashed line. The mean temperature gives us an idea of the overall \"average\" temperature throughout the year.\n",
    "- **Variance**: The variance measures how much the temperatures spread out from the mean. A higher variance indicates a wider range of temperatures. In our case, the variance suggests there is considerable spread in daily temperatures, which is expected due to seasonal changes.\n",
    "- **Standard Deviation**: This is the square root of the variance and provides a measure of temperature spread in the same units as the data itself (degrees). It tells us, on average, how much individual temperatures deviate from the mean temperature.\n",
    "\n",
    "The mean, variance, and standard deviation are fundamental for understanding the dataset's behavior. In the context of weather data analysis, these statistics can help in planning agricultural activities, energy usage forecasting, and preparing for weather-dependent events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb35430-5eab-4333-8a05-a13c17d78aef",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285f661-62d8-45c6-ab26-0844dfb33cb1",
   "metadata": {},
   "source": [
    "# 4. Simulating Dice Rolls to Understand Random Variables\n",
    "\n",
    "## Objective:\n",
    "\n",
    "Simulate rolling a six-sided die to explore the concepts of random variables, mean, and variance, and to illustrate the Law of Large Numbers.\n",
    "\n",
    "In this exercise, we simulated rolling a six-sided die with increasing numbers of trials (10, 1,000, and 10,000 rolls) to explore the concepts of random variables, mean, variance, and to illustrate the Law of Large Numbers.\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. **Simulate Dice Rolls**: Use **`numpy`** to generate random integers between 1 and 6, simulating 1,000 dice rolls.\n",
    "2. **Calculate Mean and Variance**: Compute the sample mean and variance of the outcomes to understand the distribution of dice rolls.\n",
    "3. **Repeat with Increasing Trials**: Repeat the simulation with 10, 1,000, and 10,000 rolls, plotting the mean of the outcomes against the number of trials to demonstrate the Law of Large Numbers.\n",
    "4. **Discussion**: Explain how the mean stabilizes around the theoretical mean (3.5 for a fair six-sided die) as the number of trials increases, and discuss the implications for understanding random variables and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732387b1-0a6e-49c2-b626-de856386e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Simulate Dice Rolls\n",
    "rolls_10 = np.random.randint(1, 7, 10)\n",
    "rolls_1000 = np.random.randint(1, 7, 1000)\n",
    "rolls_10000 = np.random.randint(1, 7, 10000)\n",
    "\n",
    "# Step 2: Calculate Mean and Variance\n",
    "mean_10 = np.mean(rolls_10)\n",
    "variance_10 = np.var(rolls_10)\n",
    "\n",
    "mean_1000 = np.mean(rolls_1000)\n",
    "variance_1000 = np.var(rolls_1000)\n",
    "\n",
    "mean_10000 = np.mean(rolls_10000)\n",
    "variance_10000 = np.var(rolls_10000)\n",
    "\n",
    "# Step 3: Repeat with Increasing Trials and Plot\n",
    "trial_counts = [10, 1000, 10000]\n",
    "means = [mean_10, mean_1000, mean_10000]\n",
    "variances = [variance_10, variance_1000, variance_10000]\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plotting Mean\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(trial_counts, means, marker='o', linestyle='-', color='b')\n",
    "plt.title('Mean of Dice Rolls vs. Number of Trials')\n",
    "plt.xlabel('Number of Trials')\n",
    "plt.ylabel('Mean of Outcomes')\n",
    "plt.xscale('log')  # Use logarithmic scale to better visualize the changes\n",
    "plt.axhline(y=3.5, color='r', linestyle='--')  # Theoretical mean\n",
    "plt.legend(['Experimental Mean', 'Theoretical Mean'])\n",
    "\n",
    "# Plotting Variance\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(trial_counts, variances, marker='o', linestyle='-', color='g')\n",
    "plt.title('Variance of Dice Rolls vs. Number of Trials')\n",
    "plt.xlabel('Number of Trials')\n",
    "plt.ylabel('Variance of Outcomes')\n",
    "plt.xscale('log')  # Use logarithmic scale to better visualize the changes\n",
    "plt.axhline(y=np.var(np.arange(1, 7)), color='r', linestyle='--')  # Theoretical variance\n",
    "plt.legend(['Experimental Variance', 'Theoretical Variance'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fbb06a-a7f0-4e0f-b7ed-c9e5c20734e1",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "\n",
    "- **Mean of Dice Rolls vs. Number of Trials**: The plot shows how the mean of the dice rolls approaches the theoretical mean (3.5) as the number of trials increases. This is a demonstration of the Law of Large Numbers, which states that as the number of trials increases, the sample mean will get closer to the expected (theoretical) mean of the population. Initially, with only 10 trials, the mean can deviate significantly from 3.5, but as we increase the number of rolls to 1,000 and then 10,000, the experimental mean converges towards the theoretical mean.\n",
    "- **Variance of Dice Rolls vs. Number of Trials**: The variance plot indicates the spread of the outcomes around the mean. The experimental variance approaches the theoretical variance of a fair six-sided die as the number of trials increases. The theoretical variance can be calculated from the probabilities of each outcome for a fair die, and in this exercise, it's represented by the red dashed line. Similar to the mean, the variance stabilizes as we increase the number of trials, providing a consistent measure of the outcomes' spread.\n",
    "\n",
    "This exercise demonstrates the Law of Large Numbers and the significance of mean and variance in understanding the distribution of random variables. By simulating an increasing number of dice rolls, we observed how the experimental mean and variance stabilize and converge towards their theoretical values. This not only illustrates the central concepts of statistics and probability but also underscores the importance of large sample sizes in achieving reliable and accurate estimates of population parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af1d36-425f-4afd-a6de-dd84d5558f91",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7aced6-73fa-4926-b798-018485816c6a",
   "metadata": {},
   "source": [
    "# 5. Image Data Compression Using PCA\n",
    "\n",
    "## Objective:\n",
    "\n",
    "Use Principal Component Analysis (PCA) to compress and decompress an image, illustrating the concept of variance in data compression and dimensionality reduction.\n",
    "\n",
    "## Dataset:\n",
    "\n",
    "Select a simple image or use an image from a standard dataset (e.g., MNIST if focusing on a single digit image).\n",
    "\n",
    "## Steps:\n",
    "\n",
    "1. **Prepare the Image**: Load the image and convert it to grayscale if it's in color. Flatten the image into a 2D array if necessary.\n",
    "2. **Apply PCA**: Use PCA (from **`sklearn.decomposition`**) to reduce the dimensionality of the image data, retaining different levels of variance (e.g., 95%, 90%, 85%).\n",
    "3. **Reconstruct the Image**: Inverse transform the PCA components to reconstruct the image with reduced quality.\n",
    "4. **Visualize and Compare**: Plot the original and reconstructed images side by side to visually compare the effects of data compression.\n",
    "5. **Discussion**: Discuss how the choice of variance retained affects the image quality and compression ratio, and relate this to the importance of understanding variance in data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a683d7e-4dc8-40ad-b3d6-c8574629eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('./mnist_train.csv')\n",
    "test_df = pd.read_csv('./mnist_test.csv')\n",
    "\n",
    "# Assuming the first column is the label\n",
    "X_train = train_df.drop(labels=[\"label\"], axis=1).values\n",
    "y_train = train_df[\"label\"].values\n",
    "\n",
    "# Optionally, visualize the first image\n",
    "plt.imshow(X_train[0].reshape(28, 28), cmap='gray')\n",
    "plt.title(f'Label: {y_train[0]}')\n",
    "plt.show()\n",
    "\n",
    "# Select the first image and reshape it\n",
    "image = X_train[0].reshape(28, 28)\n",
    "\n",
    "# Flatten the image for PCA\n",
    "image_flattened = image.flatten().reshape(1, -1)\n",
    "\n",
    "# Normalize pixel values\n",
    "image_flattened = image_flattened / 255.0\n",
    "\n",
    "# Check for NaNs or infinite values and replace them\n",
    "image_flattened = np.nan_to_num(image_flattened)\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "# Apply PCA with different levels of variance retained\n",
    "pca_95 = PCA(n_components=0.95)\n",
    "image_transformed_95 = pca_95.fit_transform(image_flattened)\n",
    "image_reconstructed_95 = pca_95.inverse_transform(image_transformed_95)\n",
    "\n",
    "pca_90 = PCA(n_components=0.90)\n",
    "image_transformed_90 = pca_90.fit_transform(image_flattened)\n",
    "image_reconstructed_90 = pca_90.inverse_transform(image_transformed_90)\n",
    "\n",
    "pca_85 = PCA(n_components=0.85)\n",
    "image_transformed_85 = pca_85.fit_transform(image_flattened)\n",
    "image_reconstructed_85 = pca_85.inverse_transform(image_transformed_85)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axs[0].imshow(image, cmap='gray')\n",
    "axs[0].set_title('Original Image')\n",
    "\n",
    "axs[1].imshow(image_reconstructed_95.reshape(28, 28), cmap='gray')\n",
    "axs[1].set_title('95% Variance Retained')\n",
    "\n",
    "axs[2].imshow(image_reconstructed_90.reshape(28, 28), cmap='gray')\n",
    "axs[2].set_title('90% Variance Retained')\n",
    "\n",
    "axs[3].imshow(image_reconstructed_85.reshape(28, 28), cmap='gray')\n",
    "axs[3].set_title('85% Variance Retained')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27b0dc9-212d-4dd5-ac71-4587ff142c38",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The PCA compression experiment demonstrates the trade-off between data reduction and image quality.\n",
    "- Retaining 95% of the variance preserves most details of the original image, making it almost indistinguishable from the original.\n",
    "- As we reduce the variance retained to 90% and 85%, the reconstructed images become increasingly blurry, indicating loss of detail.\n",
    "\n",
    "This exercise highlights the importance of variance in retaining information during dimensionality reduction. Choosing the right level of variance retention is crucial depending on the application's need for accuracy versus compression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
